services:
  # postgres-airflow:
  #   image: postgres:13
  #   container_name: postgres-airflow
  #   environment:
  #     POSTGRES_USER: airflow
  #     POSTGRES_PASSWORD: airflow
  #     POSTGRES_DB: airflow
  #   volumes:
  #     - ./postgres_data:/var/lib/postgresql/data
  #   restart: unless-stopped

  minio:
    image: minio/minio
    container_name: minio
    ports:
      - "1000:9000"
      - "1001:9001"
    environment:
      MINIO_ROOT_USER: minioadmin
      MINIO_ROOT_PASSWORD: minioadmin
    command: server /data --console-address ":9001"
    volumes:
      - ./minio_data:/data
    restart: unless-stopped

  # spark-master:
  #   image: bitnami/spark:latest
  #   container_name: spark-master
  #   environment:
  #     - SPARK_MODE=master
  #     - SPARK_RPC_AUTHENTICATION_ENABLED=no
  #   ports:
  #     - "1002:8080"  # Spark UI
  #     - "1003:7077"  # Spark master port
  #   volumes:
  #     - ./src:/opt/bitnami/spark/src
  #   restart: unless-stopped
  # spark-worker:
  #   image: bitnami/spark:latest
  #   container_name: spark-worker
  #   environment:
  #     - SPARK_MODE=worker
  #     - SPARK_MASTER_URL=spark://spark-master:7077
  #   depends_on:
  #     - spark-master
  #   restart: unless-stopped
    
  airflow:
    build:
      context: .      # Path to the folder containing the Dockerfile
      dockerfile: airflow/Dockerfile   # Optional if the file is literally named "Dockerfile"
    container_name: airflow
    ports:
      - "1004:8080"
    runtime: nvidia
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]
    environment:
      NVIDIA_VISIBLE_DEVICES: all
      PYTHONPATH: "/opt/airflow/plugins"
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'false'
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: sqlite:////opt/airflow/airflow.db
      AIRFLOW__CORE__SIMPLE_AUTH_MANAGER_ALL_ADMINS: "True"
      AIRFLOW_CONN_SPARK_DEFAULT: spark://spark-master:7077
      AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK: 'true'
      AIRFLOW__CORE__PARALLELISM: 8  # Reduce from 32
      AIRFLOW__OPENLINEAGE__TRANSPORT: '{"type": "http", "url": "http://fastapi:8000", "endpoint": "api/v1/lineage"}'
    env_file:
      - .env
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      # - ./airflow/scripts:/opt/airflow/scripts
      - ./airflow/requirements.txt:/requirements.txt
      - ./src:/opt/airflow/src
      - ./local_model:/content/local_model
      # - ./lineage:/lineage
      # - ./airflow/lineage_transport.py:/opt/airflow/plugins/lineage_transport.py
    entrypoint: bash -c "airflow standalone"
    # depends_on:
    #   - postgres-airflow
    restart: unless-stopped
  
  llama-server:
    image: ghcr.io/ggml-org/llama.cpp:server
    container_name: llama-server
    ports:
      - "1006:8000"
    volumes:
      - ./llama_models:/models
    command: >
      -m /models/gemma3b/gemma-3-1b.gguf
      --port 8000
      --host 0.0.0.0
      -n 512
    restart: unless-stopped

  chromadb:
    image: ghcr.io/chroma-core/chroma:latest
    container_name: chromadb
    ports:
      - "1007:8000"
    volumes:
      - ./chroma_data:/data

  fastapi:
    build:
      context: .
      dockerfile: src/Dockerfile.fastapi
    container_name: fastapi-app
    # runtime: nvidia
    # environment:
    #   - NVIDIA_VISIBLE_DEVICES=all
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - capabilities: [gpu]
    ports:
      - "1005:8000"
    volumes:
      - ./src:/app
      - ./airflow/scripts:/app/scripts
      - ./local_model:/content/local_model
    env_file:
      - .env
    depends_on:
      - minio
      - llama-server
      - chromadb
    restart: unless-stopped